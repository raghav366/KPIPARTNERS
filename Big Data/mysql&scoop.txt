[cloudera@quickstart ~]$ mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 30
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| cm                 |
| firehose           |
| hue                |
| metastore          |
| mysql              |
| nav                |
| navms              |
| oozie              |
| retail_db          |
| rman               |
| sampledb           |
| sentry             |
| zeyo_db_20         |
| zeyodb             |
+--------------------+
15 rows in set (0.01 sec)

mysql> show tables;
ERROR 1046 (3D000): No database selected

mysql> use sampledb;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+--------------------+
| Tables_in_sampledb |
+--------------------+
| emp                |
+--------------------+
1 row in set (0.00 sec)

mysql> select *from emp;
+------+-------+------+
| id   | name  | sal  |
+------+-------+------+
|  101 | Blake | 2000 |
|  102 | Adams | 2500 |
+------+-------+------+
2 rows in set (0.00 sec)



*Open another terminal for sqoop:-
[cloudera@quickstart ~]$ sqoop list-databases --connect jdbc:mysql://quickstart:3306/ --password cloudera --username root;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 00:51:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 00:51:00 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 00:51:02 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
information_schema
cm
firehose
hue
metastore
mysql
nav
navms
oozie
retail_db
rman
sampledb
sentry
zeyo_db_20
zeyodb


[cloudera@quickstart ~]$ sqoop list-tables --connect jdbc:mysql://quickstart:3306/sampledb --password cloudera --username root;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 00:55:22 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 00:55:22 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 00:55:22 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
emp


[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart:3306/sampledb --password cloudera --username root --table emp;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.


[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart:3306/sampledb --password cloudera --username root --table emp -m 1; 
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 00:58:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 00:58:09 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 00:58:10 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
22/01/11 00:58:10 INFO tool.CodeGenTool: Beginning code generation
22/01/11 00:58:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp` AS t LIMIT 1
22/01/11 00:58:12 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `emp` AS t LIMIT 1
22/01/11 00:58:12 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/9e2be4d6c66ada82627b245413d56f57/emp.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
22/01/11 00:58:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/9e2be4d6c66ada82627b245413d56f57/emp.jar
22/01/11 00:58:21 WARN manager.MySQLManager: It looks like you are importing from mysql.
22/01/11 00:58:21 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
22/01/11 00:58:21 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
22/01/11 00:58:21 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
22/01/11 00:58:21 INFO mapreduce.ImportJobBase: Beginning import of emp
22/01/11 00:58:23 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
22/01/11 00:58:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
22/01/11 00:58:27 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/192.168.245.128:8032
22/01/11 00:58:34 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 00:58:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 00:58:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 00:58:36 INFO db.DBInputFormat: Using read commited transaction isolation
22/01/11 00:58:36 INFO mapreduce.JobSubmitter: number of splits:1
22/01/11 00:58:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1641876852291_0002
22/01/11 00:58:38 INFO impl.YarnClientImpl: Submitted application application_1641876852291_0002
22/01/11 00:58:38 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1641876852291_0002/
22/01/11 00:58:38 INFO mapreduce.Job: Running job: job_1641876852291_0002
22/01/11 00:59:07 INFO mapreduce.Job: Job job_1641876852291_0002 running in uber mode : false
22/01/11 00:59:07 INFO mapreduce.Job:  map 0% reduce 0%
22/01/11 00:59:33 INFO mapreduce.Job:  map 100% reduce 0%
22/01/11 00:59:34 INFO mapreduce.Job: Job job_1641876852291_0002 completed successfully
22/01/11 00:59:34 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=171835
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=34
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=12387840
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=24195
		Total vcore-milliseconds taken by all map tasks=24195
		Total megabyte-milliseconds taken by all map tasks=12387840
	Map-Reduce Framework
		Map input records=2
		Map output records=2
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=328
		CPU time spent (ms)=3730
		Physical memory (bytes) snapshot=130220032
		Virtual memory (bytes) snapshot=1965096960
		Total committed heap usage (bytes)=35127296
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=34
22/01/11 00:59:34 INFO mapreduce.ImportJobBase: Transferred 34 bytes in 67.9202 seconds (0.5006 bytes/sec)
22/01/11 00:59:34 INFO mapreduce.ImportJobBase: Retrieved 2 records.
[cloudera@quickstart ~]$ hadoop dfs -ls
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.

Found 21 items
drwx------   - cloudera cloudera          0 2022-01-11 00:59 .staging
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:09 avro_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:15 csv_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-11 00:59 emp
drwxr-xr-x   - cloudera cloudera          0 2022-01-07 02:09 hbase
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 08:36 import_avro
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_avro_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:13 json_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:39 json_orc
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_orc_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:38 json_parquet
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 22:56 json_parquet_1
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:11 orc_dir
drwxr-xr-x   - cloudera cloudera          0 2022-01-05 22:38 output
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 22:14 parquet_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-23 23:11 parquet_json_write
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 13:40 part_dir
drwxr-xr-x   - cloudera cloudera          0 2020-05-22 14:01 part_dir2
-rw-r--r--   1 cloudera cloudera         44 2022-01-09 21:54 sample_kpi.csv
-rw-r--r--   1 cloudera cloudera         35 2022-01-05 22:26 sample_kpi.txt
drwxr-xr-x   - cloudera cloudera          0 2020-06-04 09:04 zeyo_dir
[cloudera@quickstart ~]$ hdfs dfs -ls emp;
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2022-01-11 00:59 emp/_SUCCESS
-rw-r--r--   1 cloudera cloudera         34 2022-01-11 00:59 emp/part-m-00000


[cloudera@quickstart ~]$ hdfs dfs -ls emp/part-m-00000
-rw-r--r--   1 cloudera cloudera         34 2022-01-11 00:59 emp/part-m-00000
[cloudera@quickstart ~]$ hdfs dfs -ls /user/cloudera/emp/part-m-00000
-rw-r--r--   1 cloudera cloudera         34 2022-01-11 00:59 /user/cloudera/emp/part-m-00000
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/emp/part-m-00000
101,Blake,2000.0
102,Adams,2500.0

=====================================================================================================================================


*Example- #2)
*Open a terminal for mysql:-

mysql> create table student(sid int, name varchar(20), marks real);
Query OK, 0 rows affected (0.04 sec)

mysql> describe student;
+-------+-------------+------+-----+---------+-------+
| Field | Type        | Null | Key | Default | Extra |
+-------+-------------+------+-----+---------+-------+
| sid   | int(11)     | YES  |     | NULL    |       |
| name  | varchar(20) | YES  |     | NULL    |       |
| marks | double      | YES  |     | NULL    |       |
+-------+-------------+------+-----+---------+-------+
3 rows in set (0.01 sec)

mysql> insert into student values(201, 'Emily', 86.3);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student values(202, 'Mark', 89.8);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student values(203, 'Chris', 65.87);
Query OK, 1 row affected (0.01 sec)

mysql> insert into student values(204, 'Anne', 79.98);
Query OK, 1 row affected (0.01 sec)

mysql> select * from student;
+------+-------+-------+
| sid  | name  | marks |
+------+-------+-------+
|  201 | Emily |  86.3 |
|  202 | Mark  |  89.8 |
|  203 | Chris | 65.87 |
|  204 | Anne  | 79.98 |
+------+-------+-------+
4 rows in set (0.00 sec)



*Open another one for sqoop:-
[cloudera@quickstart ~]$ sqoop import --connect jdbc:mysql://quickstart:3306/sampledb --password cloudera --username root --table student -m 1;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 01:16:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
22/01/11 01:16:59 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
22/01/11 01:17:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
22/01/11 01:17:00 INFO tool.CodeGenTool: Beginning code generation
22/01/11 01:17:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
22/01/11 01:17:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `student` AS t LIMIT 1
22/01/11 01:17:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/f3ab12e20e97593add4fda35428f709a/student.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
22/01/11 01:17:12 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/f3ab12e20e97593add4fda35428f709a/student.jar
22/01/11 01:17:12 WARN manager.MySQLManager: It looks like you are importing from mysql.
22/01/11 01:17:12 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
22/01/11 01:17:12 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
22/01/11 01:17:12 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
22/01/11 01:17:12 INFO mapreduce.ImportJobBase: Beginning import of student
22/01/11 01:17:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
22/01/11 01:17:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
22/01/11 01:17:17 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/192.168.245.128:8032
22/01/11 01:17:20 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:21 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:21 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:22 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeInternal(DFSOutputStream.java:935)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:931)
22/01/11 01:17:22 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:22 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:24 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:24 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:24 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 01:17:25 INFO db.DBInputFormat: Using read commited transaction isolation
22/01/11 01:17:25 INFO mapreduce.JobSubmitter: number of splits:1
22/01/11 01:17:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1641876852291_0003
22/01/11 01:17:26 INFO impl.YarnClientImpl: Submitted application application_1641876852291_0003
22/01/11 01:17:26 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1641876852291_0003/
22/01/11 01:17:26 INFO mapreduce.Job: Running job: job_1641876852291_0003
22/01/11 01:17:47 INFO mapreduce.Job: Job job_1641876852291_0003 running in uber mode : false
22/01/11 01:17:48 INFO mapreduce.Job:  map 0% reduce 0%
22/01/11 01:18:09 INFO mapreduce.Job:  map 100% reduce 0%
22/01/11 01:18:10 INFO mapreduce.Job: Job job_1641876852291_0003 completed successfully
22/01/11 01:18:11 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=171862
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=87
		HDFS: Number of bytes written=60
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Other local map tasks=1
		Total time spent by all maps in occupied slots (ms)=10185216
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19893
		Total vcore-milliseconds taken by all map tasks=19893
		Total megabyte-milliseconds taken by all map tasks=10185216
	Map-Reduce Framework
		Map input records=4
		Map output records=4
		Input split bytes=87
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=498
		CPU time spent (ms)=3400
		Physical memory (bytes) snapshot=130936832
		Virtual memory (bytes) snapshot=1965150208
		Total committed heap usage (bytes)=42991616
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=60
22/01/11 01:18:11 INFO mapreduce.ImportJobBase: Transferred 60 bytes in 54.1148 seconds (1.1088 bytes/sec)
22/01/11 01:18:11 INFO mapreduce.ImportJobBase: Retrieved 4 records.
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/student;
cat: `/user/cloudera/student': Is a directory
[cloudera@quickstart ~]$ hdfs dfs -ls student;
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2022-01-11 01:18 student/_SUCCESS
-rw-r--r--   1 cloudera cloudera         60 2022-01-11 01:18 student/part-m-00000
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/student/part-m-00000;
201,Emily,86.3
202,Mark,89.8
203,Chris,65.87
204,Anne,79.98

------------------------------==============================================================================----------------------------------------------

*Exporting data from sqoop to mysql:-
*Open the mysql terminal==>

mysql> create table person(name varchar(10), age int, gender varchar(10));
Query OK, 0 rows affected (0.02 sec)

mysql> describe perosn;
ERROR 1146 (42S02): Table 'sampledb.perosn' doesn't exist
mysql> describe person;
+--------+-------------+------+-----+---------+-------+
| Field  | Type        | Null | Key | Default | Extra |
+--------+-------------+------+-----+---------+-------+
| name   | varchar(10) | YES  |     | NULL    |       |
| age    | int(11)     | YES  |     | NULL    |       |
| gender | varchar(10) | YES  |     | NULL    |       |
+--------+-------------+------+-----+---------+-------+
3 rows in set (0.00 sec)

mysql> select * from person;
Empty set (0.00 sec)


*Open sqoop terminal:-
==> Create a csv file
[cloudera@quickstart ~]$ ls *.csv
sample_kpi.csv
[cloudera@quickstart ~]$ cat sample_kpi.csv
sam,24,m
emma,21,f
richard,23,m
susane,22,f



[cloudera@quickstart ~]$ sqoop export --connect jdbc:mysql://quickstart:3306/sampledb --username root -P --table person --export-dir /user/cloudera/sample_kpi.csv -m 1;
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
22/01/11 02:07:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.13.0
Enter password: 
22/01/11 02:07:18 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
22/01/11 02:07:18 INFO tool.CodeGenTool: Beginning code generation
22/01/11 02:07:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `person` AS t LIMIT 1
22/01/11 02:07:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `person` AS t LIMIT 1
22/01/11 02:07:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/0edb4fc9c174c7a910da000390c08649/person.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
22/01/11 02:07:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/0edb4fc9c174c7a910da000390c08649/person.jar
22/01/11 02:07:27 INFO mapreduce.ExportJobBase: Beginning export of person
22/01/11 02:07:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
22/01/11 02:07:32 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
22/01/11 02:07:33 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
22/01/11 02:07:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
22/01/11 02:07:34 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/192.168.245.128:8032
22/01/11 02:07:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 02:07:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 02:07:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 02:07:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 02:07:36 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)
22/01/11 02:07:38 WARN hdfs.DFSClient: DataStreamer Exception
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:475)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.DataOutputStream.flush(DataOutputStream.java:123)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:856)
22/01/11 02:07:39 INFO input.FileInputFormat: Total input paths to process : 1
22/01/11 02:07:39 INFO input.FileInputFormat: Total input paths to process : 1
22/01/11 02:07:39 INFO mapreduce.JobSubmitter: number of splits:1
22/01/11 02:07:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1641876852291_0004
22/01/11 02:07:41 INFO impl.YarnClientImpl: Submitted application application_1641876852291_0004
22/01/11 02:07:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1641876852291_0004/
22/01/11 02:07:41 INFO mapreduce.Job: Running job: job_1641876852291_0004
22/01/11 02:08:04 INFO mapreduce.Job: Job job_1641876852291_0004 running in uber mode : false
22/01/11 02:08:04 INFO mapreduce.Job:  map 0% reduce 0%
22/01/11 02:08:28 INFO mapreduce.Job:  map 100% reduce 0%
22/01/11 02:08:29 INFO mapreduce.Job: Job job_1641876852291_0004 completed successfully
22/01/11 02:08:30 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=171813
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=188
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9909760
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19355
		Total vcore-milliseconds taken by all map tasks=19355
		Total megabyte-milliseconds taken by all map tasks=9909760
	Map-Reduce Framework
		Map input records=4
		Map output records=4
		Input split bytes=141
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=256
		CPU time spent (ms)=2330
		Physical memory (bytes) snapshot=124874752
		Virtual memory (bytes) snapshot=1960292352
		Total committed heap usage (bytes)=33554432
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
22/01/11 02:08:30 INFO mapreduce.ExportJobBase: Transferred 188 bytes in 56.6971 seconds (3.3159 bytes/sec)
22/01/11 02:08:30 INFO mapreduce.ExportJobBase: Exported 4 records.



*Go to mysql terminal to check the exported records:-
mysql> select * from person;
+---------+------+--------+
| name    | age  | gender |
+---------+------+--------+
| sam     |   24 | m      |
| emma    |   21 | f      |
| richard |   23 | m      |
| susane  |   22 | f      |
+---------+------+--------+
4 rows in set (0.00 sec)
